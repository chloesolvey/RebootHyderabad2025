# ğŸš€ 404dnf_repo

> **Automated SQL & BigQuery Analytics Pipeline**
>
> *Analyze, optimize, forecast, and monitor your data workloads in Google BigQuery. Recommendations, costs, sustainability metrics, and operational insights are delivered directly to Looker Studio dashboards.*

---

## âœ¨ Features

- **SQL Analysis & Recommendations**
  - ğŸ“‚ Scans all files in the `sql/` directory for new and updated queries.
  - âœ… Performs syntax checks, dry runs, estimates query cost, bytes processed, and COâ‚‚ emitted.
  - ğŸ’¡ Generates actionable optimization recommendations.

- **Cost, Usage, and Sustainability Forecasting**
  - ğŸ“Š Loads results into a BigQuery table for historical insights.
  - ğŸ”® ML models (see `ml_models/`) forecast usage, cost, and emissions.

- **BigQuery Usage Monitoring**
  - ğŸ›¡ï¸ Monitors BigQuery service logs via a log router.
  - ğŸ“ˆ Produces usage metrics, detects anomalies, and extracts trends.

- **Looker Studio Reporting**
  - ğŸ“Š All insights, KPIs, and forecasts are analyzed and visualized in beautiful Looker Studio dashboards.

---

## ğŸ—‚ï¸ Project Structure

404dnf_repo/
â”œâ”€â”€ alerts/
â”‚ â””â”€â”€ alerting_logic.py # BigQuery usage monitors & alert logic
â”œâ”€â”€ ddl/
â”‚ â”œâ”€â”€ account_details.sql
â”‚ â”œâ”€â”€ customer_details.sql
â”‚ â””â”€â”€ transaction_details.sql # Table schema examples
â”œâ”€â”€ infra/
â”œâ”€â”€ ml_models/ # ML forecasting scripts & models
â”œâ”€â”€ optimization/
â”œâ”€â”€ project_docs/
â”œâ”€â”€ service_accounts/
â”œâ”€â”€ sql/ # MAIN: All SQL files analyzed
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ recommendation/
â”‚ â”‚ â”œâ”€â”€ init.py
â”‚ â”‚ â”œâ”€â”€ analysis_utils.py # Query checks, dry run logic, etc.
â”‚ â”‚ â”œâ”€â”€ bigquery_utils.py # GCP & BQ integration helpers
â”‚ â”‚ â””â”€â”€ file_utils.py # FS utilities
â”‚ â”œâ”€â”€ main.py # ğŸš© Main application workflow
â”‚ â””â”€â”€ tests/
â”‚ â”œâ”€â”€ tests_analysis_utils.py
â”‚ â”œâ”€â”€ tests_bigquery_utils.py
â”‚ â””â”€â”€ tests_file_utils.py
â”œâ”€â”€ cloudbuild.yaml # CI/CD pipeline config
â”œâ”€â”€ Dockerfile # Containerization
â”œâ”€â”€ essential_commands.txt
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md


---

## ğŸ”§ Getting Started

1. **Install dependencies:**
pip3 install -r requirements.txt


2. **Set up Google Cloud credentials** with permissions for BigQuery, Artifact Registry, Cloud Run, and Logging.

3. **Run the analysis pipeline locally:**
python3 -m src.main


---

## ğŸš¦ Automated Deployment â€” CI/CD & Cloud Run

- **Cloud Build Trigger:** Watches the `sql/` directory.  
ğŸ—ï¸ On every new commit/change in `sql/`:
 1. The Docker container is built (`Dockerfile`).
 2. The image is pushed to **Google Artifact Registry**.
 3. A **Cloud Run Job** is created/updated **and** executed, running the entire pipeline using your latest code and queries.
 4. All new results are loaded into BigQuery for reporting and forecasting.

**cloudbuild.yaml** (snippet):
steps:

name: 'gcr.io/cloud-builders/docker'
args: ['build', '-t', 'REGION-docker.pkg.dev/PROJECT_ID/REPO_NAME/image-name:$COMMIT_SHA', '.']

name: 'gcr.io/cloud-builders/docker'
args: ['push', 'REGION-docker.pkg.dev/PROJECT_ID/REPO_NAME/image-name:$COMMIT_SHA']

name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
entrypoint: 'gcloud'
args:
[
'run', 'jobs', 'deploy', 'bq-analysis-job',
'--image', 'REGION-docker.pkg.dev/PROJECT_ID/REPO_NAME/image-name:$COMMIT_SHA',
'--region', 'YOUR_REGION',
'--project', '$PROJECT_ID'
]

name: 'gcr.io/google.com/cloudsdktool/cloud-sdk'
entrypoint: 'gcloud'
args:
[
'run', 'jobs', 'execute', 'bq-analysis-job',
'--region', 'YOUR_REGION',
'--project', '$PROJECT_ID'
]
options:
logging: CLOUD_LOGGING_ONLY

- Replace `REGION`, `PROJECT_ID`, `REPO_NAME`, and `YOUR_REGION` appropriately.

---

## ğŸ“Š Data & Reporting Flow

- ğŸ“¥ **Analysis & Recommendations:** All SQL checked, costs & sustainability estimated.
- ğŸ¦ **BigQuery Data Lake:** All results stored for insights and further ML modeling.
- ğŸ“ˆ **Looker Studio:** Visualize insights, cost trends, forecasting, and environmental impact.
- ğŸ›¡ï¸ **Operational Monitoring:** All BigQuery activity and anomalies are tracked and alertable.

---

## ğŸ¤ Contributing

1. Fork the repository and branch off your feature.
2. Add or update tests in `src/tests/`.
3. Open a pull request with a concise description of your additions.

---

## ğŸ›¡ï¸ License

LTC_reboot_2025_404dnf_engg

---

**Professional-grade analytics, FinOps, and cost/sustainability governance for your cloud data stack â€” powered by Python & Google Cloud.**

*For setup help, see `essential_commands.txt` or documentation in `project_docs/`. For support or ideas, open a GitHub issue!*
